{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/rt-polaritydata/rt-polarity-utf8.pos', 'r') as f:\n",
    "    raw_positive_sentences = f.readlines()\n",
    "    raw_positive_sentences = [raw_sentence.replace(' \\n', '') for raw_sentence in raw_positive_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/rt-polaritydata/rt-polarity-utf8.neg', 'r') as f:\n",
    "    raw_negative_sentences = f.readlines()\n",
    "    raw_negative_sentences = [raw_sentence.replace(' \\n', '') for raw_sentence in raw_negative_sentences]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _string_cleaner(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for all datasets except for SST.\n",
    "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    return string.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_positive_sentences = [_string_cleaner(sentence) for sentence in raw_positive_sentences]\n",
    "raw_negative_sentences = [_string_cleaner(sentence) for sentence in raw_negative_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = ['<unk>']\n",
    "\n",
    "for sentence in raw_positive_sentences:\n",
    "    for word in sentence.split(\" \"):\n",
    "        if word not in vocab:\n",
    "            vocab.append(word)\n",
    "\n",
    "for sentence in raw_negative_sentences:\n",
    "    for word in sentence.split(\" \"):\n",
    "        if word not in vocab:\n",
    "            vocab.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18765"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_sentences = []\n",
    "negative_sentences = []\n",
    "positive_labels = []\n",
    "negative_labels = []\n",
    "all_sentences = []\n",
    "all_labels = []\n",
    "sentence_length = 0\n",
    "\n",
    "for sentence in raw_positive_sentences:\n",
    "    s = []\n",
    "    for word in sentence.split(\" \"):\n",
    "        s.append(vocab.index(word))\n",
    "    positive_sentences.append(s)\n",
    "    positive_labels.append(1)\n",
    "    all_sentences.append(s)\n",
    "    all_labels.append(1)\n",
    "\n",
    "for sentence in raw_negative_sentences:\n",
    "    s = []\n",
    "    for word in sentence.split(\" \"):\n",
    "        s.append(vocab.index(word))\n",
    "    negative_sentences.append(s)\n",
    "    negative_labels.append(0)\n",
    "    all_sentences.append(s)\n",
    "    all_labels.append(0)\n",
    "    \n",
    "for sentence in positive_sentences:\n",
    "    if sentence_length < len(sentence):\n",
    "        sentence_length = len(sentence)\n",
    "        \n",
    "for sentence in negative_sentences:\n",
    "    if sentence_length < len(sentence):\n",
    "        sentence_length = len(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6, 1, 7, 8, 9, 10, 11, 12, 13, 14, 9, 15, 5, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]\n",
      "the rock is destined to be the 21st century 's new conan and that he 's going to make a splash even greater than arnold schwarzenegger , jean claud van damme or steven segal\n",
      "56\n"
     ]
    }
   ],
   "source": [
    "print(positive_sentences[0])\n",
    "print(raw_positive_sentences[0])\n",
    "print(sentence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def paddinger(sentence_list, sentence_length):\n",
    "    for sentence in sentence_list:\n",
    "        if len(sentence) != sentence_length:\n",
    "            padding_list = [0 for _ in range(0, sentence_length - len(sentence))]\n",
    "            sentence.extend(padding_list)\n",
    "    return sentence_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_sentences = paddinger(positive_sentences, sentence_length)\n",
    "negative_sentences = paddinger(negative_sentences, sentence_length)\n",
    "all_sentences = paddinger(all_sentences, sentence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_list = [i for i in range(0, len(all_sentences))]\n",
    "\n",
    "random.seed(0)\n",
    "random.shuffle(index_list)\n",
    "\n",
    "shuffled_sentence = []\n",
    "shuffled_labels = []\n",
    "for index in index_list:\n",
    "    shuffled_sentence.append(all_sentences[index])\n",
    "    shuffled_labels.append(all_labels[index])\n",
    "    \n",
    "counter = 0\n",
    "train_data = []\n",
    "test_data = []\n",
    "train_label = []\n",
    "test_label = []\n",
    "\n",
    "for sentence in shuffled_sentence:\n",
    "    if counter < (len(shuffled_sentence) - (len(shuffled_sentence) * 0.01)):\n",
    "            train_data.append(sentence)\n",
    "            counter += 1\n",
    "    else:\n",
    "        test_data.append(sentence)\n",
    "        counter += 1\n",
    "\n",
    "counter = 0\n",
    "for label in shuffled_labels:\n",
    "    if counter < (len(shuffled_labels) - (len(shuffled_labels) * 0.01)):\n",
    "        train_label.append(label)\n",
    "        counter += 1\n",
    "    else:\n",
    "        test_label.append(label)\n",
    "        counter += 1\n",
    "sample_data = [train_data, test_data]\n",
    "sample_label = [train_label, test_label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data: 10556, label: 10556\n"
     ]
    }
   ],
   "source": [
    "train_data = np.array(sample_data[0])\n",
    "train_label = np.array(sample_label[0])\n",
    "print(\"data: {}, label: {}\".format(len(train_data), len(train_label)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word enbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 56, 128)           2401920   \n",
      "_________________________________________________________________\n",
      "unified_gru_6 (UnifiedGRU)   (None, 56, 32)            15552     \n",
      "_________________________________________________________________\n",
      "unified_gru_7 (UnifiedGRU)   (None, 32)                6336      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 2,423,841\n",
      "Trainable params: 2,423,841\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 128\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "model = keras.Sequential([\n",
    "    layers.Embedding(vocab_size, embedding_dim, input_length=sentence_length),\n",
    "    layers.GRU(units=32, return_sequences=True),\n",
    "    layers.GRU(units=32),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8444 samples, validate on 2112 samples\n",
      "Epoch 1/30\n",
      "8444/8444 [==============================] - 5s 625us/sample - loss: 0.6933 - accuracy: 0.5019 - val_loss: 0.6937 - val_accuracy: 0.5019\n",
      "Epoch 2/30\n",
      "8444/8444 [==============================] - 4s 485us/sample - loss: 0.6942 - accuracy: 0.4908 - val_loss: 0.6932 - val_accuracy: 0.4981\n",
      "Epoch 3/30\n",
      "8444/8444 [==============================] - 4s 505us/sample - loss: 0.6934 - accuracy: 0.4870 - val_loss: 0.6931 - val_accuracy: 0.5019\n",
      "Epoch 4/30\n",
      "8444/8444 [==============================] - 4s 485us/sample - loss: 0.6932 - accuracy: 0.4909 - val_loss: 0.6931 - val_accuracy: 0.5019\n",
      "Epoch 5/30\n",
      "8444/8444 [==============================] - 4s 487us/sample - loss: 0.6932 - accuracy: 0.4987 - val_loss: 0.6932 - val_accuracy: 0.4981\n",
      "Epoch 6/30\n",
      "8444/8444 [==============================] - 4s 498us/sample - loss: 0.6932 - accuracy: 0.4949 - val_loss: 0.6932 - val_accuracy: 0.4981\n",
      "Epoch 7/30\n",
      "8444/8444 [==============================] - 4s 505us/sample - loss: 0.6932 - accuracy: 0.5018 - val_loss: 0.6931 - val_accuracy: 0.5019\n",
      "Epoch 8/30\n",
      "8444/8444 [==============================] - 4s 498us/sample - loss: 0.6932 - accuracy: 0.4923 - val_loss: 0.6931 - val_accuracy: 0.5009\n",
      "Epoch 9/30\n",
      "8444/8444 [==============================] - 4s 501us/sample - loss: 0.6931 - accuracy: 0.4986 - val_loss: 0.6932 - val_accuracy: 0.4986\n",
      "Epoch 10/30\n",
      "8444/8444 [==============================] - 4s 500us/sample - loss: 0.6929 - accuracy: 0.5046 - val_loss: 0.6935 - val_accuracy: 0.5019\n",
      "Epoch 11/30\n",
      "8444/8444 [==============================] - 5s 550us/sample - loss: 0.6913 - accuracy: 0.5179 - val_loss: 0.6819 - val_accuracy: 0.5128\n",
      "Epoch 12/30\n",
      "8444/8444 [==============================] - 4s 525us/sample - loss: 0.6247 - accuracy: 0.6602 - val_loss: 0.5906 - val_accuracy: 0.7254\n",
      "Epoch 13/30\n",
      "8444/8444 [==============================] - 4s 506us/sample - loss: 0.4330 - accuracy: 0.8250 - val_loss: 0.5562 - val_accuracy: 0.7486\n",
      "Epoch 14/30\n",
      "8444/8444 [==============================] - 4s 510us/sample - loss: 0.2481 - accuracy: 0.9128 - val_loss: 0.5780 - val_accuracy: 0.7614\n",
      "Epoch 15/30\n",
      "8444/8444 [==============================] - 4s 510us/sample - loss: 0.1343 - accuracy: 0.9596 - val_loss: 0.7238 - val_accuracy: 0.7552\n",
      "Epoch 16/30\n",
      "8444/8444 [==============================] - 4s 512us/sample - loss: 0.0766 - accuracy: 0.9790 - val_loss: 0.7391 - val_accuracy: 0.7614\n",
      "Epoch 17/30\n",
      "8444/8444 [==============================] - 5s 534us/sample - loss: 0.0422 - accuracy: 0.9902 - val_loss: 0.9176 - val_accuracy: 0.7609\n",
      "Epoch 18/30\n",
      "8444/8444 [==============================] - 4s 498us/sample - loss: 0.0255 - accuracy: 0.9949 - val_loss: 1.0385 - val_accuracy: 0.7599\n",
      "Epoch 19/30\n",
      "8444/8444 [==============================] - 4s 519us/sample - loss: 0.0190 - accuracy: 0.9964 - val_loss: 1.1299 - val_accuracy: 0.7604\n",
      "Epoch 20/30\n",
      "8444/8444 [==============================] - 4s 528us/sample - loss: 0.0144 - accuracy: 0.9979 - val_loss: 1.1777 - val_accuracy: 0.7609\n",
      "Epoch 21/30\n",
      "8444/8444 [==============================] - 4s 509us/sample - loss: 0.0124 - accuracy: 0.9981 - val_loss: 1.2124 - val_accuracy: 0.7609\n",
      "Epoch 22/30\n",
      "8444/8444 [==============================] - 4s 519us/sample - loss: 0.0109 - accuracy: 0.9986 - val_loss: 1.2599 - val_accuracy: 0.7576\n",
      "Epoch 23/30\n",
      "8444/8444 [==============================] - 4s 502us/sample - loss: 0.0095 - accuracy: 0.9987 - val_loss: 1.3137 - val_accuracy: 0.7595\n",
      "Epoch 24/30\n",
      "8444/8444 [==============================] - 5s 554us/sample - loss: 0.0078 - accuracy: 0.9989 - val_loss: 1.3076 - val_accuracy: 0.7628\n",
      "Epoch 25/30\n",
      "8444/8444 [==============================] - 4s 495us/sample - loss: 0.0073 - accuracy: 0.9991 - val_loss: 1.3308 - val_accuracy: 0.7637\n",
      "Epoch 26/30\n",
      "8444/8444 [==============================] - 4s 511us/sample - loss: 0.0066 - accuracy: 0.9992 - val_loss: 1.3725 - val_accuracy: 0.7656\n",
      "Epoch 27/30\n",
      "8444/8444 [==============================] - 5s 546us/sample - loss: 0.0065 - accuracy: 0.9992 - val_loss: 1.3832 - val_accuracy: 0.7633\n",
      "Epoch 28/30\n",
      "8444/8444 [==============================] - 5s 558us/sample - loss: 0.0064 - accuracy: 0.9992 - val_loss: 1.3941 - val_accuracy: 0.7609\n",
      "Epoch 29/30\n",
      "8444/8444 [==============================] - 6s 660us/sample - loss: 0.0056 - accuracy: 0.9993 - val_loss: 1.4358 - val_accuracy: 0.7623\n",
      "Epoch 30/30\n",
      "8444/8444 [==============================] - 4s 498us/sample - loss: 0.0049 - accuracy: 0.9994 - val_loss: 1.4392 - val_accuracy: 0.7599\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "history = model.fit(train_data, train_label, epochs=30, batch_size=256, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18765, 128)\n"
     ]
    }
   ],
   "source": [
    "e = model.layers[0]\n",
    "weights = e.get_weights()[0]\n",
    "print(weights.shape) # shape: (vocab_size, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "out_v = io.open('vecs.tsv', 'w', encoding='utf-8')\n",
    "out_m = io.open('meta.tsv', 'w', encoding='utf-8')\n",
    "for word_num in range(vocab_size):\n",
    "    word = vocab[word_num]\n",
    "    embeddings = weights[word_num]\n",
    "    out_m.write(word + \"\\n\")\n",
    "    out_v.write('\\t'.join([str(x) for x in embeddings]) + \"\\n\")\n",
    "out_v.close()\n",
    "out_m.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-2.0",
   "language": "python",
   "name": "tf-2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
