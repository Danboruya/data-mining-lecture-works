{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/rt-polaritydata/rt-polarity-utf8.pos', 'r') as f:\n",
    "    raw_positive_sentences = f.readlines()\n",
    "    raw_positive_sentences = [raw_sentence.replace(' \\n', '') for raw_sentence in raw_positive_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/rt-polaritydata/rt-polarity-utf8.neg', 'r') as f:\n",
    "    raw_negative_sentences = f.readlines()\n",
    "    raw_negative_sentences = [raw_sentence.replace(' \\n', '') for raw_sentence in raw_negative_sentences]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _string_cleaner(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for all datasets except for SST.\n",
    "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    return string.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_positive_sentences = [_string_cleaner(sentence) for sentence in raw_positive_sentences]\n",
    "raw_negative_sentences = [_string_cleaner(sentence) for sentence in raw_negative_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = ['<unk>']\n",
    "\n",
    "for sentence in raw_positive_sentences:\n",
    "    for word in sentence.split(\" \"):\n",
    "        if word not in vocab:\n",
    "            vocab.append(word)\n",
    "\n",
    "for sentence in raw_negative_sentences:\n",
    "    for word in sentence.split(\" \"):\n",
    "        if word not in vocab:\n",
    "            vocab.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18765"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_sentences = []\n",
    "negative_sentences = []\n",
    "positive_labels = []\n",
    "negative_labels = []\n",
    "all_sentences = []\n",
    "all_labels = []\n",
    "sentence_length = 0\n",
    "\n",
    "for sentence in raw_positive_sentences:\n",
    "    s = []\n",
    "    for word in sentence.split(\" \"):\n",
    "        s.append(vocab.index(word))\n",
    "    positive_sentences.append(s)\n",
    "    positive_labels.append(1)\n",
    "    all_sentences.append(s)\n",
    "    all_labels.append(1)\n",
    "\n",
    "for sentence in raw_negative_sentences:\n",
    "    s = []\n",
    "    for word in sentence.split(\" \"):\n",
    "        s.append(vocab.index(word))\n",
    "    negative_sentences.append(s)\n",
    "    negative_labels.append(0)\n",
    "    all_sentences.append(s)\n",
    "    all_labels.append(0)\n",
    "    \n",
    "for sentence in positive_sentences:\n",
    "    if sentence_length < len(sentence):\n",
    "        sentence_length = len(sentence)\n",
    "        \n",
    "for sentence in negative_sentences:\n",
    "    if sentence_length < len(sentence):\n",
    "        sentence_length = len(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6, 1, 7, 8, 9, 10, 11, 12, 13, 14, 9, 15, 5, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]\n",
      "the rock is destined to be the 21st century 's new conan and that he 's going to make a splash even greater than arnold schwarzenegger , jean claud van damme or steven segal\n",
      "56\n"
     ]
    }
   ],
   "source": [
    "print(positive_sentences[0])\n",
    "print(raw_positive_sentences[0])\n",
    "print(sentence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def paddinger(sentence_list, sentence_length):\n",
    "    for sentence in sentence_list:\n",
    "        if len(sentence) != sentence_length:\n",
    "            padding_list = [0 for _ in range(0, sentence_length - len(sentence))]\n",
    "            sentence.extend(padding_list)\n",
    "    return sentence_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_sentences = paddinger(positive_sentences, sentence_length)\n",
    "negative_sentences = paddinger(negative_sentences, sentence_length)\n",
    "all_sentences = paddinger(all_sentences, sentence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_list = [i for i in range(0, len(all_sentences))]\n",
    "\n",
    "random.seed(0)\n",
    "random.shuffle(index_list)\n",
    "\n",
    "shuffled_sentence = []\n",
    "shuffled_labels = []\n",
    "for index in index_list:\n",
    "    shuffled_sentence.append(all_sentences[index])\n",
    "    shuffled_labels.append(all_labels[index])\n",
    "    \n",
    "counter = 0\n",
    "train_data = []\n",
    "test_data = []\n",
    "train_label = []\n",
    "test_label = []\n",
    "\n",
    "for sentence in shuffled_sentence:\n",
    "    if counter < (len(shuffled_sentence) - (len(shuffled_sentence) * 0.01)):\n",
    "            train_data.append(sentence)\n",
    "            counter += 1\n",
    "    else:\n",
    "        test_data.append(sentence)\n",
    "        counter += 1\n",
    "\n",
    "counter = 0\n",
    "for label in shuffled_labels:\n",
    "    if counter < (len(shuffled_labels) - (len(shuffled_labels) * 0.01)):\n",
    "        train_label.append(label)\n",
    "        counter += 1\n",
    "    else:\n",
    "        test_label.append(label)\n",
    "        counter += 1\n",
    "sample_data = [train_data, test_data]\n",
    "sample_label = [train_label, test_label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data: 10556, label: 10556\n"
     ]
    }
   ],
   "source": [
    "train_data = np.array(sample_data[0])\n",
    "train_label = np.array(sample_label[0])\n",
    "print(\"data: {}, label: {}\".format(len(train_data), len(train_label)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word enbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 56, 128)           2401920   \n",
      "_________________________________________________________________\n",
      "unified_gru (UnifiedGRU)     (None, 56, 32)            15552     \n",
      "_________________________________________________________________\n",
      "unified_gru_1 (UnifiedGRU)   (None, 32)                6336      \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 2,423,841\n",
      "Trainable params: 2,423,841\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 128\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "model = keras.Sequential([\n",
    "    layers.Embedding(vocab_size, embedding_dim, input_length=sentence_length),\n",
    "    layers.GRU(units=32, return_sequences=True),\n",
    "    layers.GRU(units=32),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8444 samples, validate on 2112 samples\n",
      "Epoch 1/30\n",
      "8444/8444 [==============================] - 6s 696us/sample - loss: 0.6933 - accuracy: 0.5066 - val_loss: 0.6932 - val_accuracy: 0.4981\n",
      "Epoch 2/30\n",
      "8444/8444 [==============================] - 6s 653us/sample - loss: 0.6932 - accuracy: 0.5007 - val_loss: 0.6937 - val_accuracy: 0.4981\n",
      "Epoch 3/30\n",
      "8444/8444 [==============================] - 6s 709us/sample - loss: 0.6934 - accuracy: 0.4995 - val_loss: 0.6932 - val_accuracy: 0.5019\n",
      "Epoch 4/30\n",
      "8444/8444 [==============================] - 8s 922us/sample - loss: 0.6932 - accuracy: 0.5019 - val_loss: 0.6933 - val_accuracy: 0.4981\n",
      "Epoch 5/30\n",
      "8444/8444 [==============================] - 5s 595us/sample - loss: 0.6932 - accuracy: 0.5007 - val_loss: 0.6931 - val_accuracy: 0.5014\n",
      "Epoch 6/30\n",
      "8444/8444 [==============================] - 5s 575us/sample - loss: 0.6932 - accuracy: 0.4996 - val_loss: 0.6931 - val_accuracy: 0.5019\n",
      "Epoch 7/30\n",
      "8444/8444 [==============================] - 4s 477us/sample - loss: 0.6932 - accuracy: 0.4996 - val_loss: 0.6931 - val_accuracy: 0.5014\n",
      "Epoch 8/30\n",
      "8444/8444 [==============================] - 4s 486us/sample - loss: 0.6931 - accuracy: 0.4996 - val_loss: 0.6932 - val_accuracy: 0.4976\n",
      "Epoch 9/30\n",
      "8444/8444 [==============================] - 4s 527us/sample - loss: 0.6927 - accuracy: 0.5077 - val_loss: 0.6909 - val_accuracy: 0.5166\n",
      "Epoch 10/30\n",
      "8444/8444 [==============================] - 4s 463us/sample - loss: 0.6224 - accuracy: 0.6466 - val_loss: 0.5407 - val_accuracy: 0.7453\n",
      "Epoch 11/30\n",
      "8444/8444 [==============================] - 4s 532us/sample - loss: 0.3467 - accuracy: 0.8630 - val_loss: 0.5365 - val_accuracy: 0.7604\n",
      "Epoch 12/30\n",
      "8444/8444 [==============================] - 4s 497us/sample - loss: 0.1705 - accuracy: 0.9416 - val_loss: 0.6222 - val_accuracy: 0.7642\n",
      "Epoch 13/30\n",
      "8444/8444 [==============================] - 4s 465us/sample - loss: 0.0953 - accuracy: 0.9732 - val_loss: 0.7201 - val_accuracy: 0.7637\n",
      "Epoch 14/30\n",
      "8444/8444 [==============================] - 5s 538us/sample - loss: 0.0567 - accuracy: 0.9853 - val_loss: 0.8918 - val_accuracy: 0.7623\n",
      "Epoch 15/30\n",
      "8444/8444 [==============================] - 7s 842us/sample - loss: 0.0416 - accuracy: 0.9896 - val_loss: 0.9816 - val_accuracy: 0.7547\n",
      "Epoch 16/30\n",
      "8444/8444 [==============================] - 6s 676us/sample - loss: 0.0277 - accuracy: 0.9941 - val_loss: 1.0602 - val_accuracy: 0.7538\n",
      "Epoch 17/30\n",
      "8444/8444 [==============================] - 4s 510us/sample - loss: 0.0233 - accuracy: 0.9955 - val_loss: 1.0366 - val_accuracy: 0.7562\n",
      "Epoch 18/30\n",
      "8444/8444 [==============================] - 5s 567us/sample - loss: 0.0178 - accuracy: 0.9968 - val_loss: 1.1999 - val_accuracy: 0.7557\n",
      "Epoch 19/30\n",
      "8444/8444 [==============================] - 4s 530us/sample - loss: 0.0134 - accuracy: 0.9981 - val_loss: 1.1895 - val_accuracy: 0.7543\n",
      "Epoch 20/30\n",
      "8444/8444 [==============================] - 6s 679us/sample - loss: 0.0136 - accuracy: 0.9969 - val_loss: 1.3086 - val_accuracy: 0.7552\n",
      "Epoch 21/30\n",
      "8444/8444 [==============================] - 5s 533us/sample - loss: 0.0121 - accuracy: 0.9979 - val_loss: 1.2839 - val_accuracy: 0.7547\n",
      "Epoch 22/30\n",
      "8444/8444 [==============================] - 5s 542us/sample - loss: 0.0117 - accuracy: 0.9976 - val_loss: 1.3354 - val_accuracy: 0.7491\n",
      "Epoch 23/30\n",
      "8444/8444 [==============================] - 4s 488us/sample - loss: 0.0109 - accuracy: 0.9979 - val_loss: 1.2791 - val_accuracy: 0.7476\n",
      "Epoch 24/30\n",
      "8444/8444 [==============================] - 4s 468us/sample - loss: 0.0110 - accuracy: 0.9977 - val_loss: 1.3703 - val_accuracy: 0.7509\n",
      "Epoch 25/30\n",
      "8444/8444 [==============================] - 5s 577us/sample - loss: 0.0100 - accuracy: 0.9981 - val_loss: 1.2485 - val_accuracy: 0.7514\n",
      "Epoch 26/30\n",
      "8444/8444 [==============================] - 6s 661us/sample - loss: 0.0098 - accuracy: 0.9980 - val_loss: 1.3664 - val_accuracy: 0.7476\n",
      "Epoch 27/30\n",
      "8444/8444 [==============================] - 4s 508us/sample - loss: 0.0056 - accuracy: 0.9993 - val_loss: 1.5211 - val_accuracy: 0.7491\n",
      "Epoch 28/30\n",
      "8444/8444 [==============================] - 5s 580us/sample - loss: 0.0056 - accuracy: 0.9993 - val_loss: 1.6241 - val_accuracy: 0.7476\n",
      "Epoch 29/30\n",
      "8444/8444 [==============================] - 4s 499us/sample - loss: 0.0060 - accuracy: 0.9992 - val_loss: 1.5294 - val_accuracy: 0.7533\n",
      "Epoch 30/30\n",
      "8444/8444 [==============================] - 4s 463us/sample - loss: 0.0051 - accuracy: 0.9994 - val_loss: 1.6211 - val_accuracy: 0.7505\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "history = model.fit(train_data, train_label, epochs=30, batch_size=256, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18765, 128)\n"
     ]
    }
   ],
   "source": [
    "e = model.layers[0]\n",
    "weights = e.get_weights()[0]\n",
    "print(weights.shape) # shape: (vocab_size, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "out_v = io.open('vecs.tsv', 'w', encoding='utf-8')\n",
    "out_m = io.open('meta.tsv', 'w', encoding='utf-8')\n",
    "for word_num in range(vocab_size):\n",
    "    word = vocab[word_num]\n",
    "    embeddings = weights[word_num]\n",
    "    out_m.write(word + \"\\n\")\n",
    "    out_v.write('\\t'.join([str(x) for x in embeddings]) + \"\\n\")\n",
    "out_v.close()\n",
    "out_m.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-2.0",
   "language": "python",
   "name": "tf-2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
